---

layout:   post

title:   Batch Normalization 的作用和操作

subtitle:  Batch Normalization简介

date:    2024-03-02

author:   BY LinShengfeng

header-img: img/post-bg-ios9-web.jpg

catalog: true

tags:

- 算法

- 深度学习

---

# Batch Normalization

Batch Normalization（BN）是一种在深度神经网络中广泛使用的技术，旨在通过规范化层的输入来加速训练过程，提高模型在训练集上的稳定性。批量归一化通过减少内部协变量偏移（即，网络各层输入分布的变化）来实现这一目的。

在实践中，BN 通过以下步骤对每个小批量数据进行操作：

1. 计算批次内数据的均值和方差。
2. 使用计算出的均值和方差对数据进行归一化，确保数据的均值为 0，方差为 1。
3. 引入可学习的参数，对归一化后的数据进行缩放和平移变换，以保留网络的表达能力。

BN 通常在非线性激活函数（如 ReLU）之前或之后应用，并且可以应用于任何深度网络的隐藏层。

>  **注意：**Batch Normalization 是针对输入的整个Batch样本数据的每一维特征做的，Layer Normalization 是针对单个样本的所有特征做的。在推荐系统中，每一维特征虽然值不同，但语义上是相似的；而在NLP任务中，输入的每一维含义天差地别，如有一条句子输入为"I love you."，而另一个句子为"Is it real life?"，句子与句子之间差异很大，不能对同样下标上的词作Batch Normalization，而应该对每条句子做Normalization。

## 1 计算批次内数据的均值和方差

针对输入Batch数据，对每一维都求出对应的均值$\mu$和方差$\sigma$

## 2 归一化数据

数据的归一化过程，特别是将数据的均值变为 0 和方差变为 1 的过程，是一种统计标准化方法，通常称为Z得分归一化（Z-score normalization）。这个过程能够将数据转换成具有特定均值和方差的分布，主要通过以下数学公式实现：

$$Z=\frac{(X−μ)}{σ}$$

其中：

- $X$ 是原始某特征维度的Batch数据。
- $\mu$ 是原始数据$X$的均值。
- $\sigma$是原始数据$X$的标准差。
- $Z$ 是$X$归一化后的数据。

通过对每一维特征做归一化变换，能使每一维特征的数据分布都服从均值为0，方差为1，即高斯分布。

## 3 缩放和平移变换

给定归一化后的$Z$, BN层将其转化为 $Y$, 转换过程如下:

$$Y = \gamma Z + \beta$$

其中$Z$是第二步的输出，$\gamma$是缩放因子，$\beta$是平移量，$Y$是最终的输出。

> 为什么要引入$\gamma$和$\beta$?
>
> 引入这两个参数的主要原因是为了让模型有能力恢复到原始的数据分布，如果这对于解决特定的任务是有利的。归一化操作通过将数据规范化到统一的分布来帮助加速训练和减少对初始化权重的依赖。然而，这种规范化可能会限制模型学习某些特定分布特征的能力，特别是当某些层需要非标准化数据分布以最好地执行其任务时。通过引入$\gamma$和$\beta$，模型可以通过学习这些参数的最优值，动态地调整数据的分布，既利用了归一化带来的好处，又保留了模型的灵活性和表达能力。

### 学习过程

在训练过程中，$\gamma$和$\beta$作为模型参数一起被学习。它们通过反向传播和梯度下降（或其他优化算法）不断更新，以最小化最终的损失函数。这意味着模型可以自动学习到对每个特征最合适的缩放和平移量，从而优化整体网络的性能。

