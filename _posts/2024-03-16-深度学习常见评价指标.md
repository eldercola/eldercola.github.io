---

layout:   post

title:   二分类任务常见评价指标

subtitle:  机器学习/深度学习常见的二分类任务评价指标

date:    2024-03-16

author:   BY LinShengfeng

header-img: img/post-bg-ios9-web.jpg

catalog: true

tags:

- 算法

- 深度学习

- 基础知识

- 评价指标

---

# 二分类任务常见评价指标

在接触常见评价指标前，先建立混淆矩阵的概念。

## 混淆矩阵

|                | 预测 P=1               | 预测P=0                |
| -------------- | ---------------------- | ---------------------- |
| 标签 Label = 1 | True Positive, **TP**  | False Negative, **FN** |
| 标签 Label = 0 | False Positive, **FP** | True Negative, **TN**  |

> False 是针对预测值来说，假设预测值为0，但label为1，那么预测错了，第一个单词就是False，而预测的是负样本 Negative，故预测为0，标签为1是 False Negative。同理，预测为1，实际为0，那么就是 False Positive

## 准确率、查准率、查全率

### 准确率 Accuracy

$$Accuracy = \frac{TP+TN}{TP+FN+FP+TN}$$

直观的理解是，预测值和标签一致的样本占所有样本的比例。

### 查准率 Percision (P)

算法预测的正样本中，有多少是真正的正样本。

$$Percision=\frac{TP}{TP+FP}$$

分母为所有的**预测**正样本数，分子为预测正样本中的真正的正样本数。

### 查全率 Recall (R)

$$Recall=\frac{TP}{TP+FN}$$

分母为所有的正样本数，分子为所有正样本中预测为正样本的数目。

> 在某些情况下，提高查准率会降低查全率，反之亦然。例如，如果你提高模型判断为正例的标准（使得模型更加“严格”），那么虽然可以减少假正例数量（从而提高查准率），但同时也可能会错过一些真正例（降低查全率）。相反，如果放宽判断标准，虽然可以捕获更多真正例（提高查全率），但同时也会增加假正例数量（降低查准率）。
>
> 这种情况下的权衡通常被称为**查准率-查全率权衡**（Precision-Recall Tradeoff）。在实际应用中，选择合适的查准率和查全率平衡点，往往取决于具体问题的需求。例如，在某些医疗诊断应用中，可能会更倾向于高查全率以确保尽可能少地错过任何正例；而在某些内容过滤应用中，则可能更倾向于高查准率以避免将过多不相关内容误判为正例。

## F1-Score

$$F1=2\frac{P * R}{P + R}$$

**F1分数**是一种同时考虑查准率和查全率的指标，通过计算它们的调和平均值来提供一个单一的性能度量。

## 真正率TPR、假正率FPR

### TPR

即查全率

### FPR

$$FPR=\frac{FP}{FP+TN}$$

> 记忆小技巧：TPR或FPR，分子跟前两者一样，分母就是分子+分子的全反（例如TP的全反就是FN）

------

非常好的参考链接：[二分类任务指标扫盲](https://zhuanlan.zhihu.com/p/669838554)
